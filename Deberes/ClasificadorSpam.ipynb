{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7ec7ab-4ac2-4b86-b0f5-bcee05708625",
   "metadata": {},
   "source": [
    "# 01PAO25-25 - Python, Data Types\n",
    "\n",
    "![Instituto Superior Tecnológico Quito](Recurso-26.png)\n",
    "\n",
    "**Nombre:** Germán Del Río  \n",
    "**Fecha:** 21/07/2055  \n",
    "\n",
    "---\n",
    "\n",
    "![Python Logo](python_logo.png)\n",
    "\n",
    "[-- Enlace al Repositorio](https://github.com/Saquis/MachineLearning/tree/main/Deberes )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae6b550-4dd4-48f7-a82f-05af0af9d9cf",
   "metadata": {},
   "source": [
    "# Regresión Logística: Detección de SPAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f91f0-1cfc-44c2-a1e2-2981242a959d",
   "metadata": {},
   "source": [
    "En este ejercicio se muestran los fundamentos de la Regresión Logística planteando uno de los primeros problemas que fueron solucionados mediante el uso de técnicas de Machine Learning: la detección de SPAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bf9938-27b8-43a3-a212-9d5af13619f7",
   "metadata": {},
   "source": [
    "Se propone la construcción de un sistema de aprendizaje automático capaz de predecir si un correo determinado se corresponde con un correo de SPAM o no, para ello, se utilizará el siguiente conjunto de datos:\n",
    "\n",
    "2007 TREC Public Spam Corpus\n",
    "The corpus trec07p contains 75,419 messages:\n",
    "\n",
    "25220 ham\n",
    "50199 spam\n",
    "These messages constitute all the messages delivered to a particular server between these dates:\n",
    "\n",
    "Sun, 8 Apr 2007 13:07:21 -0400\n",
    "Fri, 6 Jul 2007 07:04:53 -0400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba2ddc7-e74b-44cd-bc66-6a52c5d65e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\anaconda\\envs\\machinelearnig\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\anaconda\\envs\\machinelearnig\\lib\\site-packages (from scikit-learn) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\anaconda\\envs\\machinelearnig\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda\\envs\\machinelearnig\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda\\envs\\machinelearnig\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: joblib in c:\\anaconda\\envs\\machinelearnig\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\anaconda\\envs\\machinelearnig\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 16.4 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------------------------------------- 0/4 [tqdm]\n",
      "   ---------- ----------------------------- 1/4 [regex]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   -------------------- ------------------- 2/4 [click]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ------------------------------ --------- 3/4 [nltk]\n",
      "   ---------------------------------------- 4/4 [nltk]\n",
      "\n",
      "Successfully installed click-8.2.1 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    }
   ],
   "source": [
    "# Instalación de librerías externas\n",
    "!pip install scikit-learn\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1623c23f-18aa-4bc1-bedd-43001ffa1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Funciones complementarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32503dca-2bbd-4084-8eba-6301d892c5c1",
   "metadata": {},
   "source": [
    "En este caso práctico relacionado con la detección de correos electrónicos de SPAM, el conjunto de datos que disponemos esta formado por correos electrónicos, con sus correspondientes cabeceras y campos adicionales. Por lo tanto, requieren un preprocesamiento previo a que sean ingeridos por el algoritmo de Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51cb4bea-42c1-43a2-9cbf-7b44b3463d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta clase facilita el preprocesamiento de correos electrónicos que poseen código HTML\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MLStripper(HTMLParser):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.strict = False\n",
    "        self.convert_charrefs = True\n",
    "        self.fed = []\n",
    "\n",
    "    def handle_data(self, d):\n",
    "        self.fed.append(d)\n",
    "\n",
    "    def get_data(self):\n",
    "        return ''.join(self.fed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d349e902-4f5e-4a41-856d-c9e3aeddeccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta función se encarga de elimar los tags HTML que se encuentren en el texto del correo electrónico\n",
    "def strip_tags(html):\n",
    "    s = MLStripper()\n",
    "    s.feed(html)\n",
    "    return s.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7049cb9e-f001-4f5c-b47a-186d0a656543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Phrack World News'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ejemplo de eliminación de los tags HTML de un texto\n",
    "t = '<tr><td align=\"left\"><a href=\"../../issues/51/16.html#article\">Phrack World News</a></td>'\n",
    "strip_tags(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9335a379-16db-41ac-b028-0f592ffbcc79",
   "metadata": {},
   "source": [
    "Además de eliminar los posibles tags HTML que se encuentren en el correo electrónico, deben realizarse otras acciones de preprocesamiento para evitar que los mensajes contengan ruido innecesario. Entre ellas se encuentra la eliminación de los signos de puntuación, eliminación de posibles campos del correo electrónico que no son relevantes o eliminación de los afijos de una palabra manteniendo únicamente la raiz de la misma (Stemming). La clase que se muestra a continuación realiza estas transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e09539e-680d-4898-987e-d5e7e6b358e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# Asegurarte de tener los stopwords\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "def strip_tags(html_text):\n",
    "    \"\"\"Remove HTML tags from text.\"\"\"\n",
    "    if not html_text:\n",
    "        return \"\"\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', str(html_text))\n",
    "\n",
    "# clase\n",
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.stemmer = nltk.PorterStemmer()\n",
    "        self.stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "        self.punctuation = list(string.punctuation)\n",
    "    \n",
    "    def parse(self, email_path):\n",
    "        \"\"\"Parse an email.\"\"\"\n",
    "        with open(email_path, errors='ignore') as e:\n",
    "            msg = email.message_from_file(e)\n",
    "        return None if not msg else self.get_email_content(msg)\n",
    "    \n",
    "    def get_email_content(self, msg):\n",
    "        \"\"\"Extract the email content.\"\"\"\n",
    "        subject = self.tokenize(msg['Subject']) if msg['Subject'] else []\n",
    "        body = self.get_email_body(msg.get_payload(),\n",
    "                                   msg.get_content_type())\n",
    "        content_type = msg.get_content_type()\n",
    "        # Returning the content of the email\n",
    "        return {\"subject\": subject,\n",
    "                \"body\": body,\n",
    "                \"content_type\": content_type}\n",
    "    \n",
    "    def get_email_body(self, payload, content_type):\n",
    "        \"\"\"Extract the body of the email.\"\"\"\n",
    "        body = []\n",
    "        if type(payload) is str and content_type == 'text/plain':\n",
    "            return self.tokenize(payload)\n",
    "        elif type(payload) is str and content_type == 'text/html':\n",
    "            return self.tokenize(strip_tags(payload))\n",
    "        elif type(payload) is list:\n",
    "            for p in payload:\n",
    "                body += self.get_email_body(p.get_payload(),\n",
    "                                            p.get_content_type())\n",
    "        return body\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Transform a text string in tokens. Perform two main actions,\n",
    "        clean the punctuation symbols and do stemming of the text.\"\"\"\n",
    "        if not text:  # Manejo de texto vacío o None\n",
    "            return []\n",
    "            \n",
    "        for c in self.punctuation:\n",
    "            text = text.replace(c, \"\")\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        tokens = list(filter(None, text.split(\" \")))\n",
    "        # Stemming of the tokens\n",
    "        return [self.stemmer.stem(w.lower()) for w in tokens if w.lower() not in self.stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c537275-2edf-4039-aaec-b30d15b67a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['hello', 'world', 'test', 'email']\n"
     ]
    }
   ],
   "source": [
    "# Prueba rápida\n",
    "parser = Parser()\n",
    "test_text = \"Hello world! This is a test email.\"\n",
    "tokens = parser.tokenize(test_text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f8b9929-8633-4f48-9146-1c843c418615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "DATASET_PATH = os.path.join(\"datasets\", \"trec07p\")\n",
    "\n",
    "def parse_index(path_to_index, n_elements):\n",
    "    ret_indexes = []\n",
    "    index = open(path_to_index).readlines()\n",
    "    for i in range(n_elements):\n",
    "        mail = index[i].split(\" ../\")\n",
    "        label = mail[0]\n",
    "        path = mail[1][:-1]\n",
    "        path_mail = path.split(\"/\")[-1]\n",
    "        ret_indexes.append({\"label\":label, \"email_path\":os.path.join(DATASET_PATH, os.path.join(\"data\", path_mail))})\n",
    "    return ret_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "859250a1-91e0-4eac-936f-a38b47f20460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✗ La carpeta 'res' NO existe\n",
      "Contenido del directorio actual:\n",
      "  - .ipynb_checkpoints\n",
      "  - deberdos.ipynb\n",
      "  - debertres.ipynb\n",
      "  - deberuno.ipynb\n",
      "  - pandasdeber.ipynb\n",
      "  - python_logo.png\n",
      "  - Recurso-26.png\n",
      "  - Untitled.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Verificar si la carpeta res existe\n",
    "if os.path.exists(\"res\"):\n",
    "    print(\"✓ La carpeta 'res' existe\")\n",
    "    print(\"Contenido de res/:\")\n",
    "    for item in os.listdir(\"res\"):\n",
    "        print(f\"  - {item}\")\n",
    "        \n",
    "    # Verificar si existe trec07p\n",
    "    if os.path.exists(\"res/trec07p\"):\n",
    "        print(\"\\n✓ La carpeta 'res/trec07p' existe\")\n",
    "        print(\"Contenido de res/trec07p/:\")\n",
    "        for item in os.listdir(\"res/trec07p\"):\n",
    "            print(f\"  - {item}\")\n",
    "            \n",
    "        # Verificar si existe data\n",
    "        if os.path.exists(\"res/trec07p/data\"):\n",
    "            print(\"\\n✓ La carpeta 'res/trec07p/data' existe\")\n",
    "            print(\"Contenido de res/trec07p/data/ (primeros 10 archivos):\")\n",
    "            files = os.listdir(\"res/trec07p/data\")\n",
    "            for i, item in enumerate(files[:10]):\n",
    "                print(f\"  - {item}\")\n",
    "            if len(files) > 10:\n",
    "                print(f\"  ... y {len(files) - 10} archivos más\")\n",
    "        else:\n",
    "            print(\"\\n✗ La carpeta 'res/trec07p/data' NO existe\")\n",
    "    else:\n",
    "        print(\"\\n✗ La carpeta 'res/trec07p' NO existe\")\n",
    "else:\n",
    "    print(\"✗ La carpeta 'res' NO existe\")\n",
    "    print(\"Contenido del directorio actual:\")\n",
    "    for item in os.listdir(\".\"):\n",
    "        print(f\"  - {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30844610-a6b4-44a0-abb9-eebb3067b633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
